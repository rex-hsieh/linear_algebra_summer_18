\documentclass{amsart}
%\documentclass[11pt]{article}
%\usepackage{amscls}
\linespread{1}
%\usepackage[letterpaper,top=1.2in,bottom=1in,right=1.03in,left=1.03in]{geometry}
\usepackage[letterpaper,top=1.6in,bottom=1.3in,right=1.3in,left=1.3in]{geometry}
\usepackage{titlesec}
\usepackage{lipsum}

%\usepackage{libertine}
%\usepackage[lite,subscriptcorrection,nofontinfo,amsbb,eucal]{mtpro2}
%slantedGreek

\usepackage{hyphenat}
\usepackage{graphicx}
%\usepackage{tikz}
%\usepackage{lastpage, bbding, pmboxdraw}

%\usepackage{color}
%\definecolor{darkblue}{rgb}{0,0,.6}
%\definecolor{darkred}{rgb}{.7,0,0}
%\definecolor{darkgreen}{rgb}{0,.6,0}
%\definecolor{red}{rgb}{.98,0,0}
%\usepackage[colorlinks,pagebackref,pdfusetitle,urlcolor=darkblue,citecolor=darkblue,linkcolor=darkred,bookmarksnumbered,plainpages=false]{hyperref}
%\usepackage{cleveref}


\usepackage{pgfplots}
\usepackage{mathrsfs}



\usepackage{array, booktabs, caption}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage{hhline}% http://ctan.org/pkg/hhline
\usepackage{makecell} 
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{soul}

%\usepackage{mtpro2}


%\usepackage[nottoc]{tocbibind}
\usepackage[bottom]{footmisc}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath,amsthm,amssymb}
\usepackage[normalem]{ulem}
\usepackage{relsize}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{tikz-cd}


\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{#1}}
\makeatother



\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
	\edef\arraystretch{#1}%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{*\c@MaxMatrixCols c}}
\makeatother


\allowdisplaybreaks

\title{\textsc{Assignment 4: Chapters 4 \& 5}}
\thanks{Summer 2018. Instructor: Antonios-Alexandros Robotis.}
\date{\today}


\usepackage{float}


\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textcomp}

\renewcommand\theadalign{lc}
\renewcommand\theadfont{\bfseries}
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths

\usepackage[utf8]{inputenc} %useful to type directly diacritic characters

\usepackage{nomencl}
\makenomenclature

\usepackage{accents}




\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.5pt}
\rhead{\textsc{Assignment 2}}
\lhead{\textsc{Linear Algebra}}
%\lhead{\textsc{}}
\cfoot{\thepage}

\usepackage{lmodern}

\newtheoremstyle{mytheoremstyle} % name
{\topsep}                    % Space above
{\topsep}                    % Space below
{}                   % Body font
{}                           % Indent amount
{\bfseries}                   % Theorem head font
{.}                          % Punctuation after theorem head
{.5em}                       % Space after theorem head
{}  % Theorem head spec (can be left empty, meaning ‘normal’)


%\theoremstyle{mytheoremstyle}
%
\theoremstyle{definition}
\newtheorem{definition}{Definition}%[section]
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{obs}[definition]{Observation}
\newtheorem{example}[definition]{Example}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{properties}[definition]{Properties}
\newtheorem{motivation}[definition]{Motivation}
\newtheorem{derivation}[definition]{Derivation}
\newtheorem{remark}[definition]{Remark}
\newtheorem{fact}[definition]{Fact}

\theoremstyle{definition}
\newtheorem*{solution}{Solution}

%\usepackage{caption}
%\captionsetup[figure]{labelfont=sc}
\setlist[enumerate]{font=\bfseries\sffamily}



%\addto\captionsenglish{\renewcommand*{\proofname}{\scshape Proof.}}


%\numberwithin{equation}{section}

\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\Com}{\mathbb{C}}
\DeclareMathOperator{\NN}{\mathcal{N}}
\DeclareMathOperator{\bindist}{\mathsf{B}}
\DeclareMathOperator{\DD}{D}
\DeclareMathOperator{\betadist}{Beta}
\DeclareMathOperator{\E}{\mathbf{E}}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\samplespace}{\mathcal{S}}
\DeclareMathOperator{\suchthat}{\text{ s.t. }}
\DeclareMathOperator{\summod}{\accentset{\circ}{+}}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\1}{\mathbbm{1}}
\DeclareMathOperator{\LL}{\mathscr{L}}
\DeclareMathOperator{\Imaginary}{Im}
\DeclareMathOperator{\supp}{Supp}
\DeclareMathOperator{\powerset}{\mathcal{P}}
\DeclareMathOperator{\normP}{norm}
\DeclareMathOperator{\BB}{\mathscr{B}}
\DeclareMathOperator{\contf}{\mathcal{C}}
\DeclareMathOperator{\riemannint}{\mathscr{R}}
\DeclareMathOperator{\osc}{osc}
\DeclareMathOperator{\sigmaalg}{\sigma-algebra}
\DeclareMathOperator{\MM}{\mathcal{M}}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\D}{\dif}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\PP}{\mathbf{P}}
\DeclareMathOperator{\B}{\mathcal{B}}
\DeclareMathOperator{\CC}{\mathcal{C}}
\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\col}{\mathsf{C}}
\DeclareMathOperator{\nul}{\mathsf{N}}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\newcommand{\zerov}{\mathbf{0}}
\newcommand{\Tau}{\mathcal{T}}
\newcommand{\rng}{\mathsf{R}}

%\newcommand{\vect}[1][2]{\LL(#1,#2)}
\newcommand{\Lspace}[4]{\mathscr{L}^{#1}(#2,#3,#4)}
\newcommand{\condset}[4]{\left\{ #1  : \: #2 #3 #4 \right\}}
\newcommand{\ball}[2]{B(#1,#2)}
\newcommand{\innerproduct}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\polyn}[2]{\mathcal{P}_{#1}(#2)}
\newcommand{\GL}[2]{\mathrm{GL}_{#1}(#2)}
\newcommand{\SL}[2]{\mathrm{SL}_{#1}(#2)}

%\titleformat{\section}
%{\centering\Large\normalfont\scshape}{\thesection .}{0.5em}{}
%
%\titleformat{\subsection}
%{\centering\large\normalfont\scshape}{\thesubsection .}{0.5em}{}

\renewcommand{\qedsymbol}{$\blacksquare$}

\begin{document}
	\sloppy
	\maketitle
	
	%In what follows, let
	%\begin{itemize}[itemsep=0mm]
	%	\item $V$ be an inner product space.
	%\end{itemize}
	
	This document contains suggested solutions to a selection of \textit{representative} problems (which are graded) from the current homework assignment---a couple from each subsection assigned. For \S5.1 and \S5.2, the problems assigned are not interesting (in the sense that they can be solved pretty quickly), so I only grade two representative problems from \S5.3. They do not include every last detail, but should give you enough guidance to complete each problem fully on your own. 
	
	Even though the rest of the homework assignment are checked for completion, the grader will make an attempt at pointing out glaring faults in arguments whenever they arise. As such, some general comments on the completion of this assignment are provided at the end. 
	
	Below are a few conventions are used throughout grading your assignments, along with some ground rules for grading:
	
	
	\begin{enumerate}[itemsep=.75em]
		
%		\item Raw score is given /100, but normalised to /10 on NYUClasses. The raw score makes partial marks easier to give out.
		
		\item Due to the \S5.4 problem assigned this time (which is enormous and takes up half of the available marks), completion of the homework will not be checked this time, ie. you will not be penalised if the homework turns out incomplete.
		
%		\begin{center}
%			\begin{tabular}{c | p{9cm}}
%				\textbf{Mark(s)} & \multicolumn{1}{c}{\textbf{Description}} \\
%				\hline
%				0 & Did not complete anything in the assignment / did not hand it in. \\
%				\hline
%				5 & A lighthearted attempt at a few questions are made. \\
%				\hline
%				10 & Up to half of the questions have been attempted, with some effort put in. \\
%				\hline
%				15 & More than three-fourths of the questions have been attempted, all with considerable progress. \\
%				\hline
%				%			40 & Missing attempts on only a few questions in the assignment. \\
%				%			\hline
%				20 & Attempted every question assigned, and has made considerable progress in every question.
%			\end{tabular}
%		\end{center}
	
%		\noindent As always, the actual score given depends on the amount of assignment completed, and quality of the work attempted.
		
		%	\item When grading the questions indicated below, the following scale and description are considered:
		%	
		%	\begin{center}
		%		\begin{tabular}{c | c}
		%			\textbf{Mark(s)} & \textbf{Description} \\
		%			\hline
		%			0 & Did not complete anything to solve the problem. \\
		%			2 & A mostly incorrect attempt has been made. \\
		%			4 & A somewhat correct attempt has been made. \\
		%			6 & One to a few mistakes has been made. \\
		%			8 & One to two mistakes has been made, with wrong calculations or conclusions. \\
		%			10 & Looks great!
		%		\end{tabular}
		%	\end{center}
		%	
		%	\bigskip
		%	
		%	\noindent Any odd-value scores are given at the discretion of the grader.
		
		\item Marks available and breakdown are indicated at the beginning of every question, inside square brackets. 
		
		\item Naming convention: in the following section, question indexed as $4.x.y$ should be read as ``question $y$ from chapter $4.x$ in Lay's textbook''.
		
		\item If you wish to dispute grading on question(s), please hand in your assignment to the instructor on the next homework submission day---with your comments/disputes written around the question(s).
		
	\end{enumerate}
	
	
	
	\clearpage
	
	\section*{Suggested Solutions}
	
%	\begin{itemize}
%		\item As Alekos noted, the notation $\polyn{n}{\R}$ is used to denote the space of real polynomials of degree $\leq n$ (instead of $\mathbb{P}_n$).
		Instead of Lay's notation $M_{m \times n}$ for the (vector) space of $m \times n$ matrices with entries in $\R$, the notation $\MM_{m \times n}(\R)$ is used throughout here. For square matrices, we will use the simplified notation $\MM_{n}(\R)$ to denote the set of $n \times n$ matrices with entries over $\R$.
%	\end{itemize}
	
	\bigskip
	
	
	\begin{enumerate}[itemsep = 2mm]
		\item[4.4.15] \textit{[\textbf{6}; 2 for each part---1 for T/F, 1 for correct justification]}
		\begin{enumerate}
			\item \textbf{True}. By definition.
			
			\item \textbf{False}. $[x]_{\B} = \powerset_{\B}^{-1} x$ by definition (and $\powerset_{\B}$ is invertible---check this).
			
			\item \textbf{False}. $\polyn{3}{\R} \cong \R^4$; check that the coordinatisation map $\phi : \polyn{3}{\R} \mapsto \R^4$ is a bijection.
		\end{enumerate}
		
		
		\item[4.4.16] \textit{[\textbf{6}; 2 for each part---1 for T/F, 1 for correct justification]}
		\begin{enumerate}
			\item \textbf{True}. This is illustrated in an earlier example in the textbook.
			
			
			\item \textbf{False}. $x \mapsto [x]_{\B}$ is the coordinate mapping.
			
			\item \textbf{True}. The span of two linearly independent vectors in $\R^3$ is isomorphic to $\R^2$.
		\end{enumerate}
		
		
		
		\item[4.5.30] \textit{[\textbf{6}; 2 for each part---1 for T/F, 1 for correct justification]}
		\begin{enumerate}
			\item \textbf{False}. Such $V$ has dimension strictly less than $p$.

			\item \textbf{True}. If a set of $p$ vectors spans $V$, then it is at most $p$-dimensional.

			\item \textbf{False}. In $\R^3$, $\set{\begin{bmatrix}
				0 \\ 0 \\ 2
				\end{bmatrix}, \begin{bmatrix}
				0 \\ 0 \\ 1
				\end{bmatrix}}$ is a set of linearly dependent vectors.
		\end{enumerate}
		
		
		\item[4.6.10] \textit{[\textbf{3}; 1 for stating and/or using rank-nullity correctly, 1 for correct dimension calculations, 1 for having all of the above]}
		
		We know the dimension of column space of $A$ is the column rank of matrix $A$. By rank-nullity, we know $\dim(\col(A)) + \dim(N(A)) = 6 \implies \dim(\col(A)) = 6 -5 = 1$.
		
		
		\item[4.6.12] \textit{[\textbf{3}; 1 for stating and/or using rank-nullity correctly, 1 for correct dimension calculations, 1 for having all of the above]}
		
		We know $\dim(\col(A)) = 3 $ by the same argument as above. Hence, the matrix $A$ has 3 nonzero rows. Accordingly, $\dim(\text{Row} (A)) = 3$.
		
		
		\item[4.7.13] \textit{[\textbf{7}; 1 for attempt at calculations, 2 for correct calculations and subsequent change of basis matrix, 1 for attempting to solve $[q]_{\B}$, 2 for correct calculations and coordinates, 1 for having all of the above]}
		
		It is obvious that, in the standard basis for $\polyn{2}{\R}$,
		\begin{align*}
		[b_1]_{\CC} &= \begin{bmatrix}
		1 \\ -2 \\ 1
		\end{bmatrix} \\
		%
		[b_2]_{\CC} &= \begin{bmatrix}
		3 \\ -5 \\ 4
		\end{bmatrix} \\
		%
		[b_3]_{\CC} &= \begin{bmatrix}
		0 \\ 2 \\ 3
		\end{bmatrix}
		\end{align*}
		hence the change of basis matrix is
		\begin{align*}
		\powerset_{\CC \leftarrow \B} = \begin{bmatrix}
		1 & 3 & 0 \\ -2 & -5 & 2 \\ 1 & 4 & 3
		\end{bmatrix}
		\end{align*}
		
		Now, let $q = -1 + 2t$. We observe that
		\begin{align*}
		\powerset_{\CC \leftarrow \B} \cdot [q]_{\B} = [q]_{\CC} = \begin{bmatrix}
		- 1 \\ 2 \\ 0
		\end{bmatrix}
		\end{align*}
		hence, solving the augmented system gives us $[q]_{\B}$. We then see 
		\begin{align*}
		\begin{bmatrix}%[c c c | c]
		1 & 3 & 0 & -1 \\ -2 & -5 & 2 &2 \\ 1 & 4 & 3 & 0
		\end{bmatrix} \leadsto \begin{bmatrix}
		1 & & & 5 \\ & 1 &  & -2 \\ &  & 1 & 1
		\end{bmatrix}
		\end{align*}
		As such,
		\begin{align*}
			[q]_{\B} = \begin{bmatrix}
			5 \\ -2 \\ 1
			\end{bmatrix}
		\end{align*}
		meaning $-1+2t = 5(1 - 2t + t^2) - 2(3 - 5t + 4t^2) + (2t + 3t^2)$ (which is true by inspection).
		
		\item[5.3.21] \textit{[\textbf{8}; 2 for each part---1 for T/F, 1 for correct justification]}
		
		\begin{enumerate}
			\item \textbf{False}. $D$ must be a diagonal matrix.
			
			\item \textbf{True}. In this case we can construct a $P$ which will be invertible, and find the corresponding $D$ that makes the factorisation possible.
			
			\item \textbf{False}. Such diagonalisable matrix always has $n$ eigenvalues, counting multiplicity.
			
			\item \textbf{False}. $A$ can be diagonalisable but not invertible; an example is
			\begin{align*}
			A = \begin{bmatrix}
			1 &  \\  & \phantom{0}
			\end{bmatrix}
			\end{align*}
		\end{enumerate}
		
		
		\item[5.3.22] \textit{[\textbf{8}; 2 for each part---1 for T/F, 1 for correct justification]}
		
		\begin{enumerate}
			\item \textbf{False}. The statement is incomplete: these eigenvectors must be linear independent.
			
			
			\item \textbf{False}. It could have repeated eigenvalues as long as the basis of each eigenspace is equal to the multiplicity of that eigenvalue. The converse is true however.
			
			
			
			\item \textbf{True}. Each column of $PD$ is a column of $P$ times $A$ and is equal to the corresponding entry in $D$ times the vector $P$. This satisfies the eigenvector definition as long as the column is nonzero.
			
			
			
			\item \textbf{False}. Take the matrix
			\begin{align*}
			A = \begin{bmatrix}
			\phantom{0} & -1 \\ 1  &
			\end{bmatrix}
			\end{align*}
			which is invertible but not diagonalisable in $\R$ (since you cannot solve the characteristic equation $\lambda^2 + 1 = 0$ in $\R$). Geometrically, the matrix $A$ as defined above is a rotation matrix, and no rotation in $\R^2$ sends a nonzero vector $v$ to a scalar multiple of itself. 
		\end{enumerate}
		
		\item[5.4] \textit{[\textbf{50}; specific breakdown below]}
		
		
		Recall the commutative diagram from class:
		\begin{equation}
		\label{commutativediagram}
		\tag{$\star$}
		\begin{tikzcd}
		V\arrow[rr,"T"]\arrow[d,"\mathcal{P}_{\mathcal{B}}",swap]\arrow[dd, bend right=90,"\mathcal{P}_{\mathcal{C}}",swap]&&V\arrow[d,"\mathcal{P}_{\mathcal{B}}"]\arrow[dd, bend left=90,"\mathcal{P}_{\mathcal{C}}"]\\
		\mathbb{R}^n\arrow[rr, "\mathcal{M}(T{,} \mathcal{B})"]\arrow[d, "\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}",swap]&&\mathbb{R}^n\arrow[d, "\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}"]\\
		\mathbb{R}^n\arrow[rr,"\mathcal{M}(T{,}\mathcal{C})"]&&\mathbb{R}^n.
		\end{tikzcd}
		\end{equation}
		Here, $\mathcal{B}=\{b_1,\ldots, b_n\}$ and $\mathcal{C}=\{c_1,\ldots, c_n\}$ are bases for $V$. $\mathcal{P}_{\mathcal{B}}$ and $\mathcal{P}_{\mathcal{C}}$ are the coordinatization transformations, and $\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}$ is the transformation which changes bases. This problem will help you compute a special case of this situation. \\\\
		We will take $V=\mathcal{M}_2(\mathbb{R})$, where 
		\begin{equation*}
		\mathcal{M}_2(\mathbb{R})=\left\{
		\begin{bmatrix}
		a&b\\
		c&d
		\end{bmatrix}: a,b,c,d\in \mathbb{R}\right\}.
		\end{equation*}
		That is, $\mathcal{M}_2(\mathbb{R})$ is the set of $2\times 2$ real matrices. 
		\begin{enumerate}[label=\arabic*.]
			\item \textit{[\textbf{11}; 1 for any attempt, 1 for every axiom stated \uline{and} explained correctly]}
			
			Show that $\mathcal{M}_2(\mathbb{R})$ is a vector space over $\mathbb{R}$. That is, check \textit{all} of the vector space axioms. 
			
			\begin{solution}
				This is not difficult to verify.
				
				Obviously, the $2 \times 2$ matrix with zero entries (denoted $[0]$) is in $\MM_{2}$.
				
				For \uline{addition}, we break this down into several verifications:
				\begin{itemize}
					\item $\forall x,y \in \MM_{2}(\R), x + y = y + x$ because addition in $\R$ is commutative.
					\item $\forall x \in \MM_{2}(\R)$, $(-x)$ exists because every $r \in \R$ has an additive inverse; hence, $x+(-x) = 0 = $ zero matrix.
					\item $\forall x,y,z \in \MM_{2}(\R)$, $(x+y)+z=x+(y+z)$ because addition in $\R$ is associative.
					\item Since $0$ is the additive identity in $\R$, we have the fact that $x + [0] = [0] + x = x$ for all $x \in \MM_{2}(\R)$.
				\end{itemize}
			
				For \uline{scalar multiplication}, we break this down into several verifications again:
				\begin{itemize}
					\item $\forall x \in \MM_{2}(\R)$, $[0]x = [0]$, since $0 \cdot a = a \cdot 0 = 0 $ by the Peano axioms.
					\item $I$ (the $2 \times 2$ identity matrix) is the multiplicative identity in $\MM_{2}(\R)$, ie. $I x= xI = x$ for all $x \in \MM_{2}(\R)$.
					\item $\forall c \in \R$ and $x \in \MM_{2}(\R)$, $cx \in \MM_{2}(\R)$ because elements in $\R$ are closed under scalar multiplication.
					\item $\forall c,d \in \R$ and $x \in \MM_{2}(\R)$, $(cd)x = c(dx) $ because multiplication in $\R$ is associative.
				\end{itemize}
			
				For \uline{distributivity}, we verify the following: $\forall x,y\in \MM_{2}(\R)$ and $c,d \in \R$,
				\begin{itemize}
					\item $c(x+y) = cx + cy $ because scalar multiplication is distributive in $\R$.
					\item $(c+d)x = cx + dx $ for the same reason as above.
				\end{itemize}
			
				Together, these conclude the verification.
			\end{solution}
			
			\item \textit{[\textbf{4}; 1 for attempt of any kind, 1 for linearity, 1 for homogeneity, 1 for having all of the above]}
			
			Let $\mathcal{T}:\mathcal{M}_2(\mathbb{R})\to \mathcal{M}_2(\mathbb{R})$ be defined by $\mathcal{T}(A)=A^T$. Check that this map is a linear transformation (in the diagram above $T=\mathcal{T}$).
			
			\begin{solution}
				\begin{proof}
					We use the notation $\Tau$ throughout. For all $A,B \in \MM_{2}(\R)$ and $c \in \R$, using the fact that $\MM_{2}(\R)$ is indeed a vector space, we have
					\begin{align*}
						\Tau(A+B) &= (A+B)^\intercal = A^\intercal + B^\intercal = \Tau(A) + \Tau(B) \\
						\Tau(cA) &= (cA)^\intercal = cA^\intercal = c\Tau(A)
					\end{align*}
					as desired.
				\end{proof}
			\end{solution}
			
			\item \textit{[\textbf{6}; 1 for stating/acknowledging the definition of a basis, 2 for verifying $\B$ is a basis, 2 for verifying $\mathcal{C}$ is a basis, 1 for having all of the above]}
			
			Define $\mathcal{B}$ and $\mathcal{C}$ by
			\begin{equation*}
			\mathcal{B}=\left\{
			\begin{bmatrix}
			1&0\\
			0&0
			\end{bmatrix},
			\begin{bmatrix}
			0&1\\
			0&0
			\end{bmatrix},
			\begin{bmatrix}
			0&0\\
			1&0
			\end{bmatrix},
			\begin{bmatrix}
			0&0\\
			0&1
			\end{bmatrix}\right\},
			\end{equation*}
			\begin{equation*}
			\mathcal{C}=\left\{
			\begin{bmatrix}
			1&0\\
			0&0
			\end{bmatrix},
			\begin{bmatrix}
			1&1\\
			0&0
			\end{bmatrix},
			\begin{bmatrix}
			1&1\\
			1&0
			\end{bmatrix},
			\begin{bmatrix}
			1&1\\
			1&1
			\end{bmatrix}\right\}.
			\end{equation*}
			Check that these are bases for $\mathcal{M}_2(\mathbb{R})$. 
			
			
			\begin{solution}
				In the order listed above, let $\B = \set{b_1,b_2,b_3,b_4}$ and $\CC = \set{c_1,c_2,c_3,c_4}$. Let $\set{a_i}_{i=1}^{4}$ be a set of scalars (in $\R$). Note that the matrix equation
				\begin{align*}
				\sum_{i=1}^{4} a_i b_i = \begin{bmatrix}
				0 & 0 \\ 0 & 0
				\end{bmatrix}
				\end{align*}
				admits only $a_1=a_2=a_3=a_4 = 0$. So $\B$ is a set of linearly independent matrices. Also, note that any $W = \begin{bmatrix}
				w & x \\ y & z 
				\end{bmatrix}$ can be written as $W = w b_1 + x b_2 + y b_3 + z b_4$, hence the set of matrices is spanning.
				
				A very similar argument can be used to argue $\CC$ defines a basis as well.
			\end{solution}
		
		
			\item \textit{[\textbf{5}; 1 for attempt of any kind, 1 for correct $\powerset_{\B}$, 1 for attempting to calculate $\powerset_{\CC}$, 1 for correct $\powerset_{\CC}$, 1 for having all of the above]}
			
			Compute 
			\begin{equation*}
			\mathcal{P}_{\mathcal{B}}\left(
			\begin{bmatrix}
			a&b\\
			c&d
			\end{bmatrix}\right)
			\end{equation*}
			and 
			\begin{equation*}
			\mathcal{P}_{\mathcal{C}}\left(
			\begin{bmatrix}
			a&b\\
			c&d
			\end{bmatrix}\right). 
			\end{equation*}
			Note that $\mathcal{P}_{\mathcal{B}},\mathcal{P}_{\mathcal{C}}:\mathcal{M}_2(\mathbb{R})\to \mathbb{R}^4$. 
			
			
			\begin{solution}
				\begin{equation}
				\label{eqcoordinatisationB}
				\powerset_{\B} \left(
				\begin{bmatrix}
				a&b\\
				c&d
				\end{bmatrix}\right) = \begin{bmatrix}
				a \\ b \\ c \\ d
				\end{bmatrix}
				\end{equation}
				is clearly derived from inspection.
				
				For $\powerset_{\CC} \left(
				\begin{bmatrix}
				a&b\\
				c&d
				\end{bmatrix}\right)$, we observe that for a set of scalars (in $\R$) $\set{a_i}_{i=1}^{4}$, 
				\begin{align*}
				\sum_{i=1}^{4} a_i c_i = \begin{bmatrix}
				a_1 + a_2 + a_3 + a_4 & a_2 + a_3 + a_4  \\ a_3 + a_4 &  a_4 
				\end{bmatrix} = \begin{bmatrix}
				a&b\\
				c&d
				\end{bmatrix}
				\end{align*}
				gives the set of solution $a_1 = a-b$, $a_2 = b-c$, $a_3 = c-d$, $a_4 = d$. Hence,
				\begin{equation}
					\label{eqcoordinatisationC}
					\powerset_{\CC} \left(
					\begin{bmatrix}
					a&b\\
					c&d
					\end{bmatrix}\right) = \begin{bmatrix}
					a-b \\ b-c \\ c-d \\ d
					\end{bmatrix}
				\end{equation}
			\end{solution}
			
			
			\item \textit{[\textbf{6}; 1 for an attempt of any kind, 1 for writing $\MM(\Tau,\B)$ or $\MM(\Tau,\CC)$ with \uline{correct} dimensions, 1 for calculating any image of a basis, 1 for correct $\MM(\Tau,\B)$, 1 for correct $\MM(\Tau,\CC)$, 1 for having all of the above]}
			
			Compute the matrix representations $\mathcal{M}(\mathcal{T},\mathcal{B})$ and $\mathcal{M}(\mathcal{T},\mathcal{C})$. 
			
			\begin{solution}
				Recall that the columns of a matrix of linear transformation is given by the image of basis vectors. Furthermore, note that the map $\Tau : \R^{n \times n} \mapsto \R^{n \times n}$ for $n=2$ in this case, so the matrix of linear transformation is of size $n^2 \times n^2$ (in this case, $4 \times 4$).
				
				Observing that $T(b_1) = b_1$, $T(b_2) = b_3$, $T(b_3) = b_2$, and $T(b_4) = b_4$, we conclude
				\begin{equation}
					\label{eqmatrixB}
					\MM(\Tau,\B) = \begin{bmatrix}
					1 & & & \\ & & 1 & \\ & 1 & & \\ & & & 1
					\end{bmatrix}
				\end{equation}
				
				It is slightly unpleasant to calculate the other matrix, though; note that $T(c_i) = c_i$ for $i=1,3,4$, but
				\begin{align*}
				T(c_2) = \begin{bmatrix}
				1 & \\ 1 &
				\end{bmatrix} = c_3 - c_2 + c_1
				\end{align*}
				hence
				\begin{equation}
					\label{eqmatrixC}
					\MM(\Tau,\CC) = \begin{bmatrix}
					1 & 1 & & \\ & -1 & & \\ & 1 & 1 & \\ & & & 1
					\end{bmatrix}
				\end{equation}
			\end{solution}
			
			
			\item \textit{[\textbf{5}; 1 for correct dimensions of the matrix, 1 for attempt at expressing $\set{b_i}$ in $\CC$-coordinates, 1 for correctly expressing two or more $b_i$ in $\CC$-coordinates, 1 for correct final matrix, 1 for having all of the above]}
			
			Compute the change of basis matrix $\mathcal{P}_{\mathcal{C}\leftarrow \mathcal{B}}$.
			
			\begin{solution}
				Let $[v]_{\CC}$ denote the expression of vector $v$ in the coordinates of $\CC$. Then, note that
				\begin{align*}
				[b_1]_{\CC} &= \left[ \begin{bmatrix}
				1&0\\
				0&0
				\end{bmatrix}\right]_{\CC} = \begin{bmatrix}
				1 \\ 0 \\ 0 \\ 0 \\
				\end{bmatrix} \\
				%
				[b_2]_{\CC} &= \left[
				\begin{bmatrix}
				0&1\\
				0&0
				\end{bmatrix} \right]_{\CC} = c_2 - c_1 = \begin{bmatrix}
				-1 \\ 1 \\ 0 \\ 0
				\end{bmatrix} \\
				%
				[b_3]_{\CC} &= \left[ \begin{bmatrix}
				0&0\\
				1&0
				\end{bmatrix} \right]_{\CC} = c_3 - c_2 = \begin{bmatrix}
				0 \\ -1 \\ 1 \\ 0 
				\end{bmatrix} \\
				%
				[b_4]_{\CC} &= \left[ \begin{bmatrix}
				0&0\\
				0&1
				\end{bmatrix} \right]_{\CC}  = c_4 - c_3 = \begin{bmatrix}
				0 \\ 0 \\ -1 \\ 1
				\end{bmatrix} 
				\end{align*}
				hence
				\begin{equation}
					\label{eqchangeofbasis}
					\mathcal{P}_{\mathcal{C}\leftarrow \mathcal{B}} = \begin{bmatrix}
					1 & -1 & & \\ & 1 & -1 & \\ & & 1 & -1 \\ & & & 1
					\end{bmatrix}
				\end{equation}
			\end{solution}
			
			\item \textit{[\textbf{3}; 1 for follow-through from earlier calculations---regardless of right or wrong, 1 for correct calculations and verification, 1 for having all of the above]}
			
			By direct computation, verify that 
			\begin{equation*}
			\mathcal{P}_{\mathcal{C}}=\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}\circ \mathcal{P}_{\mathcal{B}}.
			\end{equation*}
			
			\begin{solution}
				We verify that
				\begin{align*}
				\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}\circ \mathcal{P}_{\mathcal{B}} &= \begin{bmatrix}
				1 & -1 & & \\ & 1 & -1 & \\ & & 1 & -1 \\ & & & 1
				\end{bmatrix} \begin{bmatrix}
				a \\ b \\ c \\ d
				\end{bmatrix} \\
				&= \begin{bmatrix}
				a-b \\ b-c \\ c-d \\ d
				\end{bmatrix} = \powerset_{\CC}
				\end{align*}
			\end{solution}
			
			
			\item \textit{[\textbf{6}; 1 for follow-through from earlier calculations---regardless of right or wrong, 1 for attempting to calculate $\powerset_{\CC \leftarrow \B}^{-1}$, 1 for correct $\powerset_{\CC \leftarrow \B}^{-1}$, 1 for attempting to calculate the composition, 1 for correct calculations, 1 for having all of the above]}
			
			By direct computation, verify that 
			\begin{equation*}
			\mathcal{M}(\mathcal{T},\mathcal{C})=\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}\circ\mathcal{M}(\mathcal{T},\mathcal{B})\circ \mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}^{-1}
			\end{equation*}
			
			\begin{solution}
				It is not difficult to calculate and find that
				\begin{align*}
				\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}^{-1} = \begin{bmatrix}
				1 & 1 & 1 & 1 \\ & 1 & 1 & 1 \\ & & 1 & 1 \\ & & & 1
				\end{bmatrix}
				\end{align*}
				so we see
				\begin{align*}
					\mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}\circ\mathcal{M}(\mathcal{T},\mathcal{B})\circ \mathcal{P}_{\mathcal{C}\leftarrow\mathcal{B}}^{-1} &= \begin{bmatrix}
					1 & -1 & & \\ & 1 & -1 & \\ & & 1 & -1 \\ & & & 1
					\end{bmatrix}  \begin{bmatrix}
					1 & & & \\ & & 1 & \\ & 1 & & \\ & & & 1
					\end{bmatrix} \begin{bmatrix}
					1 & 1 & 1 & 1 \\ & 1 & 1 & 1 \\ & & 1 & 1 \\ & & & 1
					\end{bmatrix} \\
					&= \begin{bmatrix}
					1 & -1 & & \\ & 1 & -1 & \\ & & 1 & -1 \\ & & & 1
					\end{bmatrix}  \begin{bmatrix}
					1 & 1 & 1 & 1 \\ & & 1 & 1 \\ & 1 & 1 & 1 \\ & & & 1
					\end{bmatrix} \\
					&= \begin{bmatrix}
					1 & 1 & & \\ & -1 & & \\ & 1 & 1 & \\ & & & 1
					\end{bmatrix}
				\end{align*}
			\end{solution}
			
			\item \textit{[\textbf{4}; 1 for follow-through from earlier calculations---regardless of right or wrong, 1 for acknowledge of the domain and range of the composition, 1 for correct calculations and matrix representation, 1 for having all of the above]}
			
			By direct computation, verify that 
			\begin{equation*}
			\mathcal{M}(\mathcal{T},\mathcal{B})=\mathcal{P}_{\mathcal{B}}\circ \Tau \circ \mathcal{P}_{\mathcal{B}}^{-1}.
			\end{equation*}
			
			\begin{solution}
				Instead of finding the matrix representation of the map $\Tau$ and calculate the composition directly, we note that the composition $\mathcal{P}_{\mathcal{B}}\circ \Tau \circ \mathcal{P}_{\mathcal{B}}^{-1} : \R^4 \mapsto \MM_{2}(\R) \mapsto \MM_{2}(\R) \mapsto \R^4$ (refer to the commutative diagram \eqref{commutativediagram} above if you don't see this immediately). Given that $\Tau$ is the transpose map, the composition sends
				\begin{align*}
					\begin{bmatrix}
					a \\ b \\ c \\ d
					\end{bmatrix} \mapsto \begin{bmatrix}
					a&b\\
					c&d
					\end{bmatrix} \mapsto \begin{bmatrix}
					a&c\\
					b&d
					\end{bmatrix} \mapsto \begin{bmatrix}
					a \\ c \\ b \\ d
					\end{bmatrix} = \underbrace{\begin{bmatrix}
					1 & & & \\ & & 1 & \\ & 1 & & \\ & & & 1
					\end{bmatrix}}_{=\eqref{eqmatrixB}} \begin{bmatrix}
					a \\ b \\ c \\ d
					\end{bmatrix}
				\end{align*}
			\end{solution}
		\end{enumerate}
		
		\end{enumerate}
	
	\bigskip %\bigskip
	
	Below are the proofs of propositions laid out in the extra credit questions. These will be graded far more harshly than the questions from above. Marks are awarded for recognising overarching ideas in the propositions/theorems, and much less so for attempts. For Extra Credit 2), I have been instructed to be very lenient, so as long as you said something meaningful, you will get 10 marks.
	
	\begin{enumerate}
		
		\item[Extra Credit]
%		\begin{enumerate}[label=\arabic*.]
%			\item Determine a basis for $\mathbb{C}=\condset{a+bi}{a,b}{\in }{\mathbb{R}}$ as a vector space over $\mathbb{R}$. Here, 
%			\begin{equation*}
%			(a+bi)+(c+di)=(a+c)+(b+d)i
%			\end{equation*}
%			and $i^2=-1$. Using your basis, define an explicit isomorphism $T:\mathbb{C}\to\mathbb{R}^k$ for an appropriate $k\in \mathbb{N}$ and check that it \textit{is} in fact an isomorphism. 
%			
%			\item If two vector spaces $V$ and $W$ over $\mathbb{R}$ are isomorphic, $V\cong W$, then is it justified to think of them as the same vector space? Why or why not?  
%		\end{enumerate}
	
		\begin{enumerate}[label=\arabic*.]
			\item We have little choice but use $\set{1,i}$ (both are \textbf{vectors}---one can use the slightly more cumbersome notation $\{ \vec{1}, \vec{i} \}$ for the basis) as our basis of $\Com$ and $\R$. Let us first verify this is indeed a basis. Treating $i$ symbolically, we know that any $z \in \Com$ can be written uniquely as some linear combination of $1$ and $i$, ie. $z = a+bi = a1 + bi \implies z \in \Span\set{1,i}$ for all $z \in \Com \implies \Com \subset \Span\set{1,i}$. Since the span of any set of vectors is always a subspace of the ambient space, $\Span \set{1,i} \subset \Com$. Together, these imply $\Span\set{1,i} = \Com$.
			
			\indent As for linear independence, this is trivially true, since for $0 = 0 + 0i \in \Com$, $a+bi = 0 \iff (a,b) = (0,0)$. This implies linear independence.
			
			Naturally, a map we will use is a ``coordinatisation'' of $(a,b)$ in $\R^2$: we define the map
			\begin{equation}
			\label{eqCR2map}
				\fullfunction{\phi}{\Com}{\R^2}{(a,b)}{\begin{bmatrix}
					a \\ b
					\end{bmatrix}}
			\end{equation}
			We will show this is a bijective homomorphism. It is obvious that \eqref{eqCR2map} is a homomorphism: for all $z, \overline{z} \in \Com$ (without loss, let $z = a+bi$ and $\overline{z}=c + di$),
			\begin{align*}
			\phi(z + \overline{z}) &= \phi( (a+c) + (b+d)i ) = \begin{bmatrix}
			a+ c \\ b+d
			\end{bmatrix} \\
			&= \begin{bmatrix}
			a \\ b
			\end{bmatrix} + \begin{bmatrix}
			c \\ d
			\end{bmatrix} = \phi(z) + \phi(\overline{z})
			\end{align*}
			and for $r \in \R$,
			\begin{align*}
			\phi(rz) &= \phi(r(a+bi)) = \begin{bmatrix}
			ra \\ rb
			\end{bmatrix} \\
			&= r \begin{bmatrix}
			a \\ b
			\end{bmatrix} = r\phi(z)
			\end{align*}
			Now we show the map is bijective: it is clear that 
			\begin{align*}
			\ker(\phi) = \condset{z \in \Com}{\phi(z)}{=}{\begin{bmatrix}
				0 \\ 0
				\end{bmatrix}} = \set{(0,0)}
			\end{align*}
			so $\phi$ is injective. It is also clear that 
			\begin{align*}
				\im(\phi) &= \condset{v \in \R^2}{\phi(z)}{=}{v \text{ for some } z \in \Com}
			\end{align*}
			by definition of $\Com$, $a,b \in \R$, so it is obvious that $\im(\phi) = \R^2$, hence $\phi$ is surjective.
			
			
			
			\item Isomorphic vector spaces have a lot in common. For starters, they have the same dimension: it is a fact two finite-dimensional vector spaces are isomorphic iff they have the same dimension\footnote{This is not too difficult to prove. Try it when you have some time.}. As a direct consequence, we have the result that every $n$-dimensional vector space over $\R$ is isomorphic to $\R^n$.
			
			For another, isomorphisms preserve some likeable properties of any vector spaces: an isomorphism $\phi$ maps linearly independent sets to linearly independent sets, spanning sets to spanning sets, and bases to bases\footnote{None of these is hard to prove---try it if you haven't already!}. More generally, $\phi$ preserves linear combinations---so any definition or structure that depends only on linear combinations will be preserved by $\phi$.
			
			On the flip side, this also means that determinants and lengths/angles between vectors, for example, are generally not preserved.
			
			As algebraic structures, rather than geometric structures, isomorphic vector spaces can be thought of as the same object.
		\end{enumerate}
		
		
	\end{enumerate}
	
	\clearpage
	
	
	
\section*{General Comments}


The point of studying mathematics is to be able to come up with \textit{simple, nontrivial examples}, from which one can think about the fundamental principles underlying them. Some of these T/F questions help one in doing exactly that. Some of these proofs-type questions also test one's understanding of the fundamentals---including, but not limited to, definitions, basic propositions, some theorems and their consequences, and so on.

Some general comments for the homework (as a whole):
\begin{itemize}
	\item Most people did an excellent job providing counterexamples, and I was pleasantly surprised by the growing number of people who attempted to justify their responses to T/F questions.
	
	\item To those who did the extra credit question(s): they were all valiant attempts. There were good ideas in those responses, and it was good seeing that you know what needs to be proven.
\end{itemize}

\noindent Here are some comments for each question (some of which familiar from the previous homework):

\bigskip

\begin{enumerate}[itemsep = 2mm]
	\item[4.4.15]
	
	
	
	\item[4.4.16]
	
	
	
	
	\item[4.5.30]
	
	
	
	
	\item[4.6.10]
	
	
	
	\item[4.6.12]
	
	
	
	
	\item[4.7.13]
	
	
	
	
	\item[5.3.21]
	
	
	
	
	\item[5.3.22]
	
	
	
	
	\item[5.4]
	
	
	
	
	
	\item[Extra Credit]
\end{enumerate}

	
\end{document}